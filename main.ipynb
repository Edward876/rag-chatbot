{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Question  \\\n",
      "0         What is Artificial Intelligence?   \n",
      "1                What is Machine Learning?   \n",
      "2                      What is Blockchain?   \n",
      "3               What is Quantum Computing?   \n",
      "4                   What is 5G Technology?   \n",
      "...                                    ...   \n",
      "3995       What is Mobile App Development?   \n",
      "3996  What is Natural Language Processing?   \n",
      "3997               What is Edge Computing?   \n",
      "3998                 What is Data Science?   \n",
      "3999              What is Computer Vision?   \n",
      "\n",
      "                                                 Answer  \n",
      "0     Artificial Intelligence refers to a technology...  \n",
      "1     Machine Learning refers to a technology that f...  \n",
      "2     Blockchain refers to a technology that focuses...  \n",
      "3     Quantum Computing refers to a technology that ...  \n",
      "4     5G Technology refers to a technology that focu...  \n",
      "...                                                 ...  \n",
      "3995  Mobile App Development refers to a technology ...  \n",
      "3996  Natural Language Processing refers to a techno...  \n",
      "3997  Edge Computing refers to a technology that foc...  \n",
      "3998  Data Science refers to a technology that focus...  \n",
      "3999  Computer Vision refers to a technology that fo...  \n",
      "\n",
      "[4000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"tech_faq_4000_samples.csv\") \n",
    "\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "documents = [\n",
    "    Document(page_content=row[\"Answer\"], metadata={\"question\": row[\"Question\"]})\n",
    "    for _, row in df.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Split the documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\supra\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\supra\\.conda\\envs\\tf-gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\supra\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\supra\\AppData\\Local\\Temp\\ipykernel_19972\\1004806145.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Load SentenceTransformer model and HuggingFace embeddings\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create the FAISS vector store\n",
    "vectorstore = FAISS.from_documents(split_docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\supra\\.conda\\envs\\tf-gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\supra\\.cache\\huggingface\\hub\\models--google--flan-t5-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\")\n",
    "\n",
    "def generate_answer(prompt):\n",
    "    result = generator(prompt, max_length=200, num_return_sequences=1)\n",
    "    return result[0][\"generated_text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query, vectorstore, top_k=3):\n",
    "    retrieved_docs = vectorstore.similarity_search(query, k=top_k)\n",
    "    combined_content = \" \".join([doc.page_content for doc in retrieved_docs])\n",
    "    return combined_content, retrieved_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query, vectorstore):\n",
    "  \n",
    "    combined_content, retrieved_docs = retrieve_documents(query, vectorstore)\n",
    "\n",
    "    # Generate answer using retrieved context\n",
    "    prompt = f\"Answer the question based on the following context:\\n\\n{combined_content}\\n\\nQuestion: {query}\"\n",
    "    answer = generate_answer(prompt)\n",
    "\n",
    "  \n",
    "    return answer, retrieved_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: a technology that focuses on creating systems capable of simulating human intelligence\n",
      "Source Documents:\n",
      "What is Artificial Intelligence? : Artificial Intelligence refers to a technology that focuses on creating systems capable of simulating human intelligence.\n",
      "What is Artificial Intelligence? : Artificial Intelligence refers to a technology that focuses on creating systems capable of simulating human intelligence.\n",
      "What is Artificial Intelligence? : Artificial Intelligence refers to a technology that focuses on creating systems capable of simulating human intelligence.\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query = \"What is Artificial Intelligence?\"\n",
    "answer, source_docs = answer_query(query, vectorstore)\n",
    "\n",
    "# Print the generated answer\n",
    "print(\"Answer:\", answer)\n",
    "\n",
    "# Print the source documents\n",
    "print(\"Source Documents:\")\n",
    "for doc in source_docs:\n",
    "    print(doc.metadata[\"question\"], \":\", doc.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: training algorithms to learn patterns from data\n",
      "\n",
      "Source Documents:\n",
      "- What is Machine Learning?: Machine Learning refers to a technology that focuses on training algorithms to learn patterns from data.\n",
      "\n",
      "- What is Machine Learning?: Machine Learning refers to a technology that focuses on training algorithms to learn patterns from data.\n",
      "\n",
      "- What is Machine Learning?: Machine Learning refers to a technology that focuses on training algorithms to learn patterns from data.\n",
      "\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def retrieve_documents(query, vectorstore, top_k=3):\n",
    "    retrieved_docs = vectorstore.similarity_search(query, k=top_k)\n",
    "    \n",
    "    unique_docs = []\n",
    "    seen_content = set()\n",
    "    for doc in retrieved_docs:\n",
    "        if doc.page_content not in seen_content:\n",
    "            unique_docs.append(doc)\n",
    "            seen_content.add(doc.page_content)\n",
    "    return \" \".join([doc.page_content for doc in unique_docs]), unique_docs\n",
    "\n",
    "\n",
    "while True:\n",
    "    query = input(\"Ask a question (or type 'exit' to quit): \")\n",
    "    if query.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    answer, source_docs = answer_query(query, vectorstore)\n",
    "    print(f\"\\nAnswer: {answer}\\n\")\n",
    "    print(\"Source Documents:\")\n",
    "    for doc in source_docs:\n",
    "        print(f\"- {doc.metadata['question']}: {doc.page_content}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
